{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')  # this is not needed for langchain?\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "# Account for deprecation of LLM model\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().date()\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT Prompt Engineering for Developers\n",
    "# Building Systems with the ChatGPT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a garden so bright, a carrot grew,\n",
      "With a smile so wide, and a vibrant hue.\n",
      "It danced in the breeze, so full of glee,\n",
      "A happy carrot, as happy as can be!\n",
      "\n",
      "With roots in the earth, it reached for the sky,\n",
      "Its leafy greens waving, saying \"hi!\"\n",
      "It shared its joy with all who passed by,\n",
      "A happy carrot, bringing smiles to the eye!\n",
      "\n",
      "So let us remember, in life's daily grind,\n",
      "To be like the carrot, with happiness entwined.\n",
      "Spread joy and laughter, wherever we go,\n",
      "Just like the happy carrot, all in a row!\n",
      "{'prompt_tokens': 37, 'completion_tokens': 128, 'total_tokens': 165}\n"
     ]
    }
   ],
   "source": [
    "# prompting\n",
    "def get_completion_and_token_count(\n",
    "    messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=500\n",
    "):\n",
    "    # messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message[\"content\"]\n",
    "\n",
    "    token_dict = {\n",
    "        \"prompt_tokens\": response[\"usage\"][\"prompt_tokens\"],\n",
    "        \"completion_tokens\": response[\"usage\"][\"completion_tokens\"],\n",
    "        \"total_tokens\": response[\"usage\"][\"total_tokens\"],\n",
    "    }\n",
    "\n",
    "    return content, token_dict\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are an assistant who responds in the style of Dr Seuss.\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"write me a 4 sentence long poem about a happy carrot\"\"\",\n",
    "    },\n",
    "]\n",
    "response, token_dict = get_completion_and_token_count(messages)\n",
    "\n",
    "print(response)\n",
    "print(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"flagged\": false,\n",
      "  \"categories\": {\n",
      "    \"sexual\": false,\n",
      "    \"hate\": false,\n",
      "    \"harassment\": false,\n",
      "    \"self-harm\": false,\n",
      "    \"sexual/minors\": false,\n",
      "    \"hate/threatening\": false,\n",
      "    \"violence/graphic\": false,\n",
      "    \"self-harm/intent\": false,\n",
      "    \"self-harm/instructions\": false,\n",
      "    \"harassment/threatening\": false,\n",
      "    \"violence\": false\n",
      "  },\n",
      "  \"category_scores\": {\n",
      "    \"sexual\": 8.910085e-06,\n",
      "    \"hate\": 0.00013615482,\n",
      "    \"harassment\": 0.0023860661,\n",
      "    \"self-harm\": 7.5545418e-06,\n",
      "    \"sexual/minors\": 2.20487e-07,\n",
      "    \"hate/threatening\": 7.746158e-06,\n",
      "    \"violence/graphic\": 0.00012008196,\n",
      "    \"self-harm/intent\": 5.9765495e-07,\n",
      "    \"self-harm/instructions\": 3.4945369e-09,\n",
      "    \"harassment/threatening\": 0.0015225811,\n",
      "    \"violence\": 0.34292138\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# moderation\n",
    "response = openai.Moderation.create(\n",
    "    input=\"\"\"\n",
    "Here's the plan.  We get the warhead, \n",
    "and we hold the world ransom...\n",
    "...FOR ONE MILLION DOLLARS!\n",
    "\"\"\"\n",
    ")\n",
    "moderation_output = response[\"results\"][0]\n",
    "print(moderation_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain for LLM Application Development\n",
    "### Model Prompt Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompting with LangChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "customer_messages = prompt_template.format_messages(\n",
    "    style=customer_style, text=customer_email\n",
    ")\n",
    "customer_response = chat(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_template.messages[0].prompt, end=\"\\n__2__\\n\")\n",
    "print(prompt_template.messages[0].prompt.input_variables, end=\"\\n__3__\\n\")\n",
    "print(type(customer_messages), end=\"\\n__4__\\n\")\n",
    "print(type(customer_messages[0]), end=\"\\n__5__\\n\")\n",
    "print(customer_messages[0], end=\"\\n__6__\\n\")\n",
    "print(customer_messages[0].content, end=\"\\n__7__\\n\")\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputing Dictionaries/json\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]\n",
    "\n",
    "# this is not clear... duplicated!\n",
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                format_instructions=format_instructions)\n",
    "response = chat(messages)\n",
    "output_dict = output_parser.parse(response.content)  # add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_instructions, end=\"\\n__2__\\n\")\n",
    "print(messages[0].content, end=\"\\n__3__\\n\")\n",
    "print(response.content, end=\"\\n__4__\\n\")\n",
    "print(output_dict, end=\"\\n__5__\\n\")\n",
    "print(type(output_dict), end=\"\\n__6__\\n\")\n",
    "print(output_dict.get('delivery_days'), end=\"\\n__7__\\n\")\n",
    "print(output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "* <b>ConversationBufferMemory</b> - allows for storing of messages and then extracts the messages in a variable\n",
    "* <b>ConversationBufferWindowMemory</b> - keeps a list of the interactions of the conversation over time. It only uses the last K interactions\n",
    "* <b>ConversationTokenBufferMemory</b> - keeps a buffer of recent interactions in memory and uses token length rather than number of interactions to determine when to flush interactions\n",
    "* <b>ConversationSummaryMemory</b> - creates a summary of the conversation over time\n",
    "\n",
    "* <b> (optional) Vector data memory</b> - Stores text (from conversation or elsewhere) in a vector database and retrieves the most relevant blocks of text\n",
    "* <b> (optional) Entity memories</b> - Using an LLM, it remembers details about specific entities\n",
    "\n",
    "You can also use multiple memories at one time. E.g., Conversation memory + Entity memory to recall individuals\n",
    "\n",
    "You can also store the conversation in a conventional database (such as key-value store SQL)\n",
    "\n",
    "LLM are stateless - each transaction is independent (Chatbots appear to have memory by providing the full conversation as 'context')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationTokenBufferMemory,\n",
    "    ConversationSummaryBufferMemory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferMemory()\n",
    "# memory = ConversationBufferWindowMemory(k=1)\n",
    "# memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "# memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory added by conversation\n",
    "conversation.predict(input=\"Hi, my name is Andrew\")\n",
    "conversation.predict(input=\"What is 1+1?\")\n",
    "conversation.predict(input=\"What is my name?\")\n",
    "\n",
    "print(memory.buffer)\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory added by direct manipulation\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})\n",
    "print(memory.buffer)\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "\n",
    "* LLMChain\n",
    "* Sequential Chains\n",
    "  * SimpleSequentialChain\n",
    "  * SequentialChain\n",
    "* Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Data.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "product = \"Queen Size Sheet Set\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleSequentialChain (1 input/output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company: {company_name}\"\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[chain_one, chain_two], verbose=True\n",
    ")\n",
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SequentialChain (multiple inputs/outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\")\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\")\n",
    "\n",
    "\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")\n",
    "\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt, output_key=\"followup_message\")\n",
    "\n",
    "# chain 1: input=Review, output=English_Review\n",
    "# chain 2: input=English_Review, output=summary\n",
    "# chain 3: input=Review, output=language\n",
    "# chain 4: input=summary, language; output=followup_message\n",
    "# overall_chain: input=Review, output=English_Review, summary, followup_message\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "review = df.Review[5]\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router Chain\n",
    "Router Chain consists of multiple subchains, each specializing in handling a particular type of input.\n",
    "The router chain first determines the appropriate subchain for the given input and then directs it to that subchain for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]\n",
    "\n",
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "\n",
    "\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)\n",
    "\n",
    "\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"What is black body radiation?\")\n",
    "chain.run(\"what is 2 + 2\")\n",
    "chain.run(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### QnA\n",
    "### Evaluation\n",
    "### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM's are 'stateless'. Each transaction is independent. \n",
    "# Chatbots appear to have memory by providing the full conversation as 'context'\n",
    "\n",
    "# LangChain provides several kinds of 'memory' to store and accumualte the conversation\n",
    "\n",
    "# Agents with tables\n",
    "from langchain.agents import create_csv_agent\n",
    "# from langchain.agents import create_pandas_dataframe_agent  # might be the same\n",
    "# from langchain.llms import OpenAI  # without llms\n",
    "\n",
    "# # LangChain agents\n",
    "# llm = OpenAI(temperature=0)\n",
    "agent = create_csv_agent(chat, verbose=True)\n",
    "# agent = create_pandas_dataframe_agent(llm, df, verbose=True)\n",
    "# prompt = \"Make the values more readable, store the result into 'listening.csv'\"\n",
    "agent.run(customer_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with tables\n",
    "import pandas as pd\n",
    "import utils\n",
    "\n",
    "# Getting csv + df\n",
    "_, ws_dfs = utils.get_worksheets(\"Korea - Vocabulary (raw)\", (\"listening occurences\",))\n",
    "ws_df = ws_dfs[0]\n",
    "ws_df.to_csv(\"in.csv\", index=False)\n",
    "csv_df = pd.read_csv('in.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging occurences\n",
    "df = csv_df.reset_index()\n",
    "word_agg_f = lambda x: ' '.join(str(val) for val in x if pd.notnull(val))\n",
    "\n",
    "grouped_df = df.groupby(['Word']).agg(word_agg_f)\n",
    "grouped_df[\"index\"] = grouped_df[\"index\"].str.split().str[0].astype(int)\n",
    "sorted_df = grouped_df.sort_values(by='index')\n",
    "sorted_df = sorted_df.drop(['index'], axis=1)\n",
    "sorted_df.to_csv('listening.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents with tables\n",
    "from langchain.agents import create_csv_agent\n",
    "from langchain.agents import create_pandas_dataframe_agent  # might be the same\n",
    "from langchain.llms import OpenAI  # without llms\n",
    "\n",
    "# # LangChain agents\n",
    "# llm = OpenAI(temperature=0)\n",
    "# # agent = create_csv_agent(llm, filepath, verbose=True)\n",
    "# agent = create_pandas_dataframe_agent(llm, df, verbose=True)\n",
    "# prompt = \"Make the values more readable, store the result into 'listening.csv'\"\n",
    "# agent.run(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RetrievalQA (not for me for now)\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=\"out.csv\", encoding=\"utf-8\")\n",
    "data = loader.load()\n",
    "index_creator = VectorstoreIndexCreator(vectorstore_cls=DocArrayInMemorySearch)  # without par gives error\n",
    "docsearch = index_creator.from_loaders([loader])\n",
    "chain = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.vectorstore.as_retriever(), input_key=\"question\")\n",
    "\n",
    "def ask_question(query):\n",
    "    response = chain({\"question\": query})\n",
    "    return response['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Get me country with the highest GDP.\"\n",
    "query2 = \"Create 'out22.csv', where will be 3 countries with the biggest happiness index\"\n",
    "print(ask_question(query2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandasai library - not publicly common, use langchain instead\n",
    "from pandasai import PandasAI\n",
    "from pandasai.llm.openai import OpenAI\n",
    "\n",
    "# pandasai\n",
    "llm = OpenAI(api_token=\"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\")\n",
    "pandas_ai = PandasAI(llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"],\n",
    "    \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064],\n",
    "    \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12]\n",
    "})\n",
    "\n",
    "pandas_ai.run(df, prompt='Which are the 5 happiest countries?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
