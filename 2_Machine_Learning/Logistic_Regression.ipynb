{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Model Prediction (+Sigmoid or Logistic Function)\n",
    "- We would like the predictions of our classification model to be between 0 and 1 since our output variable $y$ is either 0 or 1. \n",
    "- This can be accomplished by using a \"sigmoid function\" which maps all input values to values between 0 and 1. \n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    " A logistic regression model applies the sigmoid to the familiar linear regression model as shown below:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) = \\frac{1}{1+e^{-[{w} \\cdot \\mathbf{x}^{(i)} + b]}}\\tag{3} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Loss Function\n",
    "\n",
    "It is natural to consider Linear regression's cost function for logistic regression as well. However, $f_{wb}(x)$ now has a non-linear component, the sigmoid function, and the squared error doesn't handle values from 0 to 1 well.\n",
    "\n",
    ">**Definition Note:**   In this course, these definitions are used:  \n",
    "**Loss** is a measure of the difference of a single example to its target value while the  \n",
    "**Cost** is a measure of the losses over the training set\n",
    "\n",
    "\n",
    "Logistic Regression uses a loss function more suited to the task of categorization where the target is 0 or 1 rather than any number. \n",
    "\n",
    "This is defined: \n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n",
    "\n",
    "\\begin{equation}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n",
    "    - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=1$}\\\\\n",
    "    - \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=0$}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value.\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$ where function $g$ is the sigmoid function.\n",
    "\n",
    "The defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or ($y=0$) and another for when the target is one ($y=1$). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target.\n",
    "\n",
    "The loss function above can be rewritten to be easier to implement, but might be rather formidable-looking equation.\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$\n",
    "\n",
    "## Gradient Descent for Logistic Regression\n",
    " - The gradient functions for linear and logistic regression are very similar. They differ only in the implementation of $f_{wb}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "    \"\"\"\n",
    "\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "   \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic(X, y, w, b):\n",
    "    \"\"\"Computes cost\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "\n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    total_cost = 0.0\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i],w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        total_cost += -y[i] * np.log(f_wb_i) - (1-y[i]) * np.log(1-f_wb_i)\n",
    "\n",
    "    total_cost = cost / m\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic(X, y, w, b): \n",
    "    \"\"\"Computes the gradient for linear regression\n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "    Returns\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)                           # (n,)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          # (n,)(n,) = scalar\n",
    "        err_i  = f_wb_i  - y[i]                       # scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err_i * X[i, j]      # scalar\n",
    "        dj_db += err_i\n",
    "\n",
    "    dj_dw = dj_dw / m                                   # (n,)\n",
    "    dj_db = dj_db / m                                   # scalar\n",
    "        \n",
    "    return dj_db, dj_dw\n",
    "\n",
    "# # UNQ_C3\n",
    "# # GRADED FUNCTION: compute_gradient\n",
    "# def compute_gradient(X, y, w, b, *argv): \n",
    "#     \"\"\"\n",
    "#     Computes the gradient for logistic regression \n",
    " \n",
    "#     Args:\n",
    "#       X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "#       y : (ndarray Shape (m,))  target value \n",
    "#       w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "#       b : (scalar)              value of bias parameter of the model\n",
    "#       *argv : unused, for compatibility with regularized version below\n",
    "#     Returns\n",
    "#       dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. \n",
    "#       dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b. \n",
    "#     \"\"\"\n",
    "#     m, n = X.shape\n",
    "#     dj_dw = np.zeros(w.shape)\n",
    "#     dj_db = 0.\n",
    "\n",
    "#     ### START CODE HERE ### \n",
    "#     for i in range(m):\n",
    "#         z_wb = 0\n",
    "#         for j in range(n):\n",
    "#             z_wb_ij = X[i, j] * w[j]\n",
    "#             z_wb += z_wb_ij\n",
    "#         z_wb += b\n",
    "#         f_wb = sigmoid(z_wb)\n",
    "        \n",
    "#         dj_db_i = f_wb - y[i]\n",
    "#         dj_db += dj_db_i\n",
    "        \n",
    "#         for j in range(n):\n",
    "#             dj_dw[j] += dj_db_i * X[i, j]\n",
    "            \n",
    "#     dj_dw = dj_dw / m\n",
    "#     dj_db = dj_db / m\n",
    "#     ### END CODE HERE ###\n",
    "        \n",
    "#     return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n",
    "    \"\"\"Performs batch gradient descent\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)   : Data, m examples with n features\n",
    "      y (ndarray (m,))   : target values\n",
    "      w_in (ndarray (n,)): Initial values of model parameters  \n",
    "      b_in (scalar)      : Initial values of model parameter\n",
    "      alpha (float)      : Learning rate\n",
    "      num_iters (scalar) : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,))   : Updated values of parameters\n",
    "      b (scalar)         : Updated value of parameter \n",
    "    \"\"\"\n",
    "\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_logistic(X, y, w, b) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history         #return final w,b and J history for graphing\n",
    "\n",
    "# UNQ_C4\n",
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "\n",
    "    Returns:\n",
    "      p : (ndarray (m,)) The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "   \n",
    "    ### START CODE HERE ### \n",
    "    # Loop over each example\n",
    "    for i in range(m):   \n",
    "        z_wb = 0\n",
    "        # Loop over each feature\n",
    "        for j in range(n): \n",
    "            # Add the corresponding term to z_wb\n",
    "            z_wb += X[i, j] * w[j]\n",
    "        \n",
    "        # Add bias term \n",
    "        z_wb += b\n",
    "        \n",
    "        # Calculate the prediction for this example\n",
    "        f_wb = sigmoid(z_wb)\n",
    "\n",
    "        # Apply the threshold\n",
    "        p[i] = 1 if f_wb >= 0.5 else 0\n",
    "        \n",
    "    ### END CODE HERE ### \n",
    "    return p\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
