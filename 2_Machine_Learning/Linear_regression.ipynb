{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3\"></a>\n",
    "## Model Prediction\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "Model Prediction in vector notation (where $\\cdot$ is a vector `dot product`):\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "\n",
    "<a name=\"toc_15456_4\"></a>\n",
    "## Cost function\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "\n",
    "- $f_{w,b}(x^{(i)})$ is the model's prediction for example $i$ using parameters $w,b$, while $y^{(i)}$ is the target value\n",
    "- $(f_{w,b}(x^{(i)}) -y^{(i)})^2$ is the squared difference between the target value and the prediction.\n",
    "- These differences are summed over all the $m$ examples and divided by `2m` to produce the cost, $J(w,b)$.  ($n$ is the number of features)\n",
    "- We use squared error to penalize the model for bigger differences, we are emphasizing bigger errors this way. Also in other words, a lot of small errors is better than having one big error! To build a cost function that doesn't automatically get bigger as the training set size gets larger, by convention, we will compute the average squared error instead of the total squared error by dividing by m. By convention, the cost function that machine learning people use actually divides by 2 times m. The extra division by 2 is just meant to make some of our later calculations look neater, but the cost function still works whether you include this division by 2 or not.\n",
    "\n",
    "<a name=\"toc_15456_5\"></a>\n",
    "## Gradient Descent\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "\n",
    "    for i in range(m):                                \n",
    "        f_wb_i = np.dot(X[i], w) + b\n",
    "        cost += (f_wb_i - y[i]) ** 2\n",
    "    cost /= 2 * m\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]   \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] += err * X[i, j]   # (6)\n",
    "        dj_db += err                    # (7)\n",
    "    dj_dw /= m                                \n",
    "    dj_db /= m                                \n",
    "    \n",
    "    # this is more optimized\n",
    "    # f_wb = X @ w + b              \n",
    "    # e   = f_wb - y                \n",
    "    # dj_dw  = (1/m) * (X.T @ e)\n",
    "    # dj_db  = (1/m) * np.sum(e)\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "    \"\"\"\n",
    "\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  # avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w, b)\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i < 100000:      # prevent resource exhaustion \n",
    "            J_history.append(cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}\")\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# init training set and parameters\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m2104\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m45\u001b[39m], [\u001b[39m1416\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m40\u001b[39m], [\u001b[39m852\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m35\u001b[39m]])\n\u001b[0;32m      3\u001b[0m y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m460\u001b[39m, \u001b[39m232\u001b[39m, \u001b[39m178\u001b[39m])\n\u001b[0;32m      4\u001b[0m b_init \u001b[39m=\u001b[39m \u001b[39m785.1811367994083\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# init training set and parameters\n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "\n",
    "# make a prediction\n",
    "x1 = X_train[0,:]\n",
    "y1 = y_train[0]\n",
    "f_wb = predict(x1, w_init, b_init)\n",
    "print(f\"x1 value: {x1}\")\n",
    "print(f\"prediction: {f_wb}, target value: {y1}\\n\")\n",
    "\n",
    "# compute cost\n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}\\n')\n",
    "\n",
    "# compute gradient\n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b:\\n {tmp_dj_dw}\\n')\n",
    "\n",
    "# run gradient descent (all together)\n",
    "w_init2 = np.zeros_like(w_init)\n",
    "b_init2 = 0.\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, w_init2, b_init2,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"\\nb, w found by gradient descent: {b_final:0.2f},{w_final}\")\n",
    "m = X_train.shape[0]\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These results are not inspiring*! Cost is still declining and our predictions are not very accurate. The next lab will explore how to improve on this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing Feature scaling and Learning Rate\n",
    "The lectures discussed three different techniques: \n",
    "- Scaling to the range of -1 and 1 two ways: $x_i := \\dfrac{x_i}{max} $ (for positive features), $x_i := \\dfrac{x_i - min}{max - min} $ (for any features)\n",
    "- Mean normalization: $x_i := \\dfrac{x_i - \\mu_i}{max - min} $ \n",
    "- Z-score normalization: $x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} $ (all features will have a mean of 0 and a standard deviation of 1.)\n",
    "\n",
    "where $j$ selects a feature or a column in the $\\mathbf{X}$ matrix. $Âµ_j$ is the mean of all the values for feature (j) and $\\sigma_j$ is the standard deviation of feature (j).\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5}\\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{6}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ">**Implementation Note:** When normalizing the features, it is important\n",
    "to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters from the model, we often want to predict the prices of houses we have not seen before. Given a new x value (living room area and number of bed-rooms), we must first normalize x using the mean and standard deviation that we had previously computed from the training set.\n",
    "\n",
    "**Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      X (ndarray (m, n))     : input data, m examples, n features\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m, n)): input normalized by column\n",
    "      mu (ndarray (n, ))     : mean of each feature\n",
    "      sigma (ndarray (n, ))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "\n",
    "    # find the mean of each column/feature\n",
    "    mu     = np.mean(X, axis=0)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     y \u001b[39m=\u001b[39m data[:,\u001b[39m4\u001b[39m]\n\u001b[0;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m X, y\n\u001b[1;32m----> 7\u001b[0m X_train, y_train \u001b[39m=\u001b[39m load_house_data()\n\u001b[0;32m      8\u001b[0m X_norm, X_mu, X_sigma \u001b[39m=\u001b[39m zscore_normalize_features(X_train)\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX_mu = \u001b[39m\u001b[39m{\u001b[39;00mX_mu\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mX_sigma = \u001b[39m\u001b[39m{\u001b[39;00mX_sigma\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m, in \u001b[0;36mload_house_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_house_data\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mloadtxt(\u001b[39m\"\u001b[39m\u001b[39m./data/houses.txt\u001b[39m\u001b[39m\"\u001b[39m, delimiter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, skiprows\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m     X \u001b[39m=\u001b[39m data[:,:\u001b[39m4\u001b[39m]\n\u001b[0;32m      4\u001b[0m     y \u001b[39m=\u001b[39m data[:,\u001b[39m4\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def load_house_data():\n",
    "    data = np.loadtxt(\"./data/houses.txt\", delimiter=',', skiprows=1)\n",
    "    X = data[:,:4]\n",
    "    y = data[:,4]\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = load_house_data()\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\")\n",
    "print(f\"Peak to Peak range by column in Raw        X: {np.ptp(X_train, axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X: {np.ptp(X_norm, axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit more iterations would even improve the cost even further\n",
    "w_norm, b_norm, hist = gradient_descent(X_norm, y_train, w_init2, b_init2,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    1.0e-1, 119)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m X_train, y_train \u001b[39m=\u001b[39m load_house_data()\n\u001b[0;32m      4\u001b[0m scaler \u001b[39m=\u001b[39m sklearn\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39mStandardScaler()  \u001b[39m# same as z-score\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "X_train, y_train = load_house_data()\n",
    "scaler = sklearn.preprocessing.StandardScaler()  # same as z-score\n",
    "X_norm = scaler.fit_transform(X_train)\n",
    "\n",
    "sgdr = sklearn.linear_model.SGDRegressor(max_iter=1000)\n",
    "sgdr.fit(X_norm, y_train)\n",
    "y_pred_sgd = sgdr.predict(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_norm2 = sgdr.intercept_\n",
    "w_norm2 = sgdr.coef_\n",
    "print(f\"Our model parameters:           w: {w_norm}, b: {b_norm}\")\n",
    "print(f\"Scikit-learn model parameters:  w: {w_norm2}, b:{b_norm2}\")\n",
    "\n",
    "# predict target using normalized features\n",
    "m = X_norm.shape[0]\n",
    "yp = np.zeros(m)\n",
    "for i in range(m):\n",
    "    yp[i] = np.dot(X_norm[i], w_norm) + b_norm\n",
    "print(f\"Our prediction on training set:             {yp[:4]}\")\n",
    "\n",
    "y_pred = np.dot(X_norm, w_norm2) + b_norm2  \n",
    "print(f\"Scikit-learn prediction on training set:    {y_pred[:4]}\")\n",
    "print(f\"Target values                               {y_train[:4]}\\n\")\n",
    "\n",
    "print(f\"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}\")\n",
    "print(f\"prediction using np.dot() and sgdr.predict match: {(y_pred == y_pred_sgd).all()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
