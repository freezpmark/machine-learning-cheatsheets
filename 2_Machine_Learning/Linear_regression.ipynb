{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_3\"></a>\n",
    "## Model Prediction\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "Model Prediction function for linear regression in vector notation (where $\\cdot$ is a vector `dot product`):\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "\n",
    "<a name=\"toc_15456_4\"></a>\n",
    "## Cost function\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "\n",
    "- $f_{w,b}(x^{(i)})$ is the model's prediction for example $i$ using parameters $w,b$, while $y^{(i)}$ is the target value\n",
    "- $(f_{w,b}(x^{(i)}) -y^{(i)})^2$ is the squared difference between the target value and the prediction.\n",
    "- These differences are summed over all the $m$ examples and divided by `2m` to produce the cost, $J(w,b)$.  ($n$ is the number of features)\n",
    "- We use squared error to penalize the model for bigger differences, we are emphasizing bigger errors this way. Also in other words, a lot of small errors is better than having one big error! To build a cost function that doesn't automatically get bigger as the training set size gets larger, by convention, we will compute the average squared error instead of the total squared error by dividing by m. By convention, the cost function that machine learning people use actually divides by 2 times m. The extra division by 2 is just meant to make some of our later calculations look neater, but the cost function still works whether you include this division by 2 or not.\n",
    "\n",
    "<a name=\"toc_15456_5\"></a>\n",
    "## Gradient Descent\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    total_cost = 0.0\n",
    "\n",
    "    for i in range(m):                                \n",
    "        f_wb_i = np.dot(X[i], w) + b\n",
    "        cost = (f_wb_i - y[i]) ** 2\n",
    "        total_cost += cost\n",
    "\n",
    "    total_cost /= 2 * m\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        err = (np.dot(X[i], w) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err * X[i, j]   # (6)\n",
    "        dj_db += err                    # (7)\n",
    "\n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "    \n",
    "    # this is more optimized\n",
    "    # f_wb = X @ w + b              \n",
    "    # e   = f_wb - y                \n",
    "    # dj_dw  = (1/m) * (X.T @ e)\n",
    "    # dj_db  = (1/m) * np.sum(e)\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "    \"\"\"\n",
    "\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  # avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w, b)\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i < 100000:      # prevent resource exhaustion \n",
    "            J_history.append(cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}\")\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 value: [2104    5    1   45]\n",
      "prediction: 459.9999976194083, target value: 460\n",
      "\n",
      "Cost at optimal w : 1.5578904428966628e-12\n",
      "\n",
      "dj_db at initial w,b: -1.6739251501955248e-06\n",
      "dj_dw at initial w,b:\n",
      " [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]\n",
      "\n",
      "Iteration    0: Cost  2529.46\n",
      "Iteration  100: Cost   695.99\n",
      "Iteration  200: Cost   694.92\n",
      "Iteration  300: Cost   693.86\n",
      "Iteration  400: Cost   692.81\n",
      "Iteration  500: Cost   691.77\n",
      "Iteration  600: Cost   690.73\n",
      "Iteration  700: Cost   689.71\n",
      "Iteration  800: Cost   688.70\n",
      "Iteration  900: Cost   687.69\n",
      "\n",
      "b, w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07]\n",
      "prediction: 426.19, target value: 460\n",
      "prediction: 286.17, target value: 232\n",
      "prediction: 171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# init training set and parameters\n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "\n",
    "# make a prediction\n",
    "x1 = X_train[0,:]\n",
    "y1 = y_train[0]\n",
    "f_wb = predict(x1, w_init, b_init)\n",
    "print(f\"x1 value: {x1}\")\n",
    "print(f\"prediction: {f_wb}, target value: {y1}\\n\")\n",
    "\n",
    "# compute cost\n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}\\n')\n",
    "\n",
    "# compute gradient\n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b:\\n {tmp_dj_dw}\\n')\n",
    "\n",
    "# run gradient descent (all together)\n",
    "w_init2 = np.zeros_like(w_init)\n",
    "b_init2 = 0.\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, w_init2, b_init2,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"\\nb, w found by gradient descent: {b_final:0.2f},{w_final}\")\n",
    "m = X_train.shape[0]\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These results are not inspiring*! Cost is still declining and our predictions are not very accurate. The next lab will explore how to improve on this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing Feature scaling and Learning Rate\n",
    "The lectures discussed three different techniques: \n",
    "- Scaling to the range of -1 and 1 two ways: $x_i := \\dfrac{x_i}{max} $ (for positive features), $x_i := \\dfrac{x_i - min}{max - min} $ (for any features)\n",
    "- Mean normalization: $x_i := \\dfrac{x_i - \\mu_i}{max - min} $ \n",
    "- Z-score normalization: $x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} $ (all features will have a mean of 0 and a standard deviation of 1.)\n",
    "\n",
    "where $j$ selects a feature or a column in the $\\mathbf{X}$ matrix. $Âµ_j$ is the mean of all the values for feature (j) and $\\sigma_j$ is the standard deviation of feature (j).\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5}\\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{6}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ">**Implementation Note:** When normalizing the features, it is important\n",
    "to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters from the model, we often want to predict the prices of houses we have not seen before. Given a new x value (living room area and number of bed-rooms), we must first normalize x using the mean and standard deviation that we had previously computed from the training set.\n",
    "\n",
    "**Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      X (ndarray (m, n))     : input data, m examples, n features\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m, n)): input normalized by column\n",
    "      mu (ndarray (n, ))     : mean of each feature\n",
    "      sigma (ndarray (n, ))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "\n",
    "    # find the mean of each column/feature\n",
    "    mu     = np.mean(X, axis=0)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_mu = [1.42e+03 2.72e+00 1.38e+00 3.84e+01], \n",
      "X_sigma = [411.62   0.65   0.49  25.78]\n",
      "Peak to Peak range by column in Raw        X: [2.41e+03 4.00e+00 1.00e+00 9.50e+01]\n",
      "Peak to Peak range by column in Normalized X: [5.85 6.14 2.06 3.69]\n"
     ]
    }
   ],
   "source": [
    "def load_house_data():\n",
    "    data = np.loadtxt(\"./data/houses.txt\", delimiter=',', skiprows=1)\n",
    "    X = data[:,:4]\n",
    "    y = data[:,4]\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = load_house_data()\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\")\n",
    "print(f\"Peak to Peak range by column in Raw        X: {np.ptp(X_train, axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X: {np.ptp(X_norm, axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 57617.03\n",
      "Iteration   12: Cost  5310.52\n",
      "Iteration   24: Cost   900.14\n",
      "Iteration   36: Cost   394.33\n",
      "Iteration   48: Cost   286.39\n",
      "Iteration   60: Cost   248.04\n",
      "Iteration   72: Cost   231.87\n",
      "Iteration   84: Cost   224.79\n",
      "Iteration   96: Cost   221.67\n",
      "Iteration  108: Cost   220.30\n"
     ]
    }
   ],
   "source": [
    "# a bit more iterations would even improve the cost even further\n",
    "w_norm, b_norm, hist = gradient_descent(X_norm, y_train, w_init2, b_init2,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    1.0e-1, 119)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, linear_model\n",
    "\n",
    "X_train, y_train = load_house_data()\n",
    "scaler = preprocessing.StandardScaler()  # same as z-score\n",
    "X_norm = scaler.fit_transform(X_train)\n",
    "\n",
    "sgdr = linear_model.SGDRegressor(max_iter=1000)\n",
    "sgdr.fit(X_norm, y_train)\n",
    "y_pred_sgd = sgdr.predict(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model parameters:           w: [109.13 -20.55 -31.9  -38.25], b: 363.15477778549433\n",
      "Scikit-learn model parameters:  w: [110.23 -21.09 -32.49 -38.04], b:[363.15]\n",
      "Our prediction on training set:             [295.18 485.7  389.75 491.85]\n",
      "Scikit-learn prediction on training set:    [295.15 485.95 389.6  492.11]\n",
      "Target values                               [300.  509.8 394.  540. ]\n",
      "\n",
      "number of iterations completed: 131, number of weight updates: 12970.0\n",
      "prediction using np.dot() and sgdr.predict match: True\n"
     ]
    }
   ],
   "source": [
    "b_norm2 = sgdr.intercept_\n",
    "w_norm2 = sgdr.coef_\n",
    "print(f\"Our model parameters:           w: {w_norm}, b: {b_norm}\")\n",
    "print(f\"Scikit-learn model parameters:  w: {w_norm2}, b:{b_norm2}\")\n",
    "\n",
    "# predict target using normalized features\n",
    "m = X_norm.shape[0]\n",
    "yp = np.zeros(m)\n",
    "for i in range(m):\n",
    "    yp[i] = np.dot(X_norm[i], w_norm) + b_norm\n",
    "print(f\"Our prediction on training set:             {yp[:4]}\")\n",
    "\n",
    "y_pred = np.dot(X_norm, w_norm2) + b_norm2  \n",
    "print(f\"Scikit-learn prediction on training set:    {y_pred[:4]}\")\n",
    "print(f\"Target values                               {y_train[:4]}\\n\")\n",
    "\n",
    "print(f\"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}\")\n",
    "print(f\"prediction using np.dot() and sgdr.predict match: {(y_pred == y_pred_sgd).all()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Closed-form solution based on the normal equation (does not require normalization)\n",
    "- The final parameter values and predictions are very close to the results from the un-normalized 'long-run' from that lab. That un-normalized run took hours to produce results, while this is nearly instantaneous. The closed-form solution work well on smaller data sets such as these but can be computationally demanding on larger data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = sklearn.linear_model.LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "b = linear_model.intercept_\n",
    "w = linear_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =                         [  0.27 -32.62 -67.25  -1.47], b = 220.42\n",
      "Prediction on training set: [295.18 485.98 389.52 492.15]\n",
      "prediction using w, b:      [295.18 485.98 389.52 492.15]\n",
      "Target values:              [300.  509.8 394.  540. ]\n",
      "predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = $318709.09\n"
     ]
    }
   ],
   "source": [
    "print(f\"w =                         {w:}, b = {b:0.2f}\")\n",
    "print(f\"Prediction on training set: {linear_model.predict(X_train)[:4]}\" )\n",
    "print(f\"prediction using w, b:      {(X_train @ w + b)[:4]}\")\n",
    "print(f\"Target values:              {y_train[:4]}\")\n",
    "\n",
    "x_house = np.array([1200, 3,1, 40]).reshape(-1,4)\n",
    "x_house_predict = linear_model.predict(x_house)[0]\n",
    "print(f\"predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = ${x_house_predict*1000:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
